# IntelAgentsAvatars

Course Project for CIS 6930 at University of Florida


## About this Research Project

This research explores the innovative intersection of speech recognition and facial animation, aiming to develop a highly accurate and expressive avatar system capable of generating facial 
animations directly from audio input. Using advanced machine learning models like LSTM networks, our project bridges the gap between traditional, labor-intensive animation processes and the dynamic 
capabilities of AI-driven techniques. Through the meticulous analysis of audio files, our system is designed to interpret and visualize speech-driven expressions, converting them into realistic 3D facial motions,
including nuanced lip movements and subtle expressions, across a diverse array of speech intensities and patterns.

This approach addresses the significant challenges faced by independent creators in producing animated content with high emotional and expressive fidelity, due to the prohibitive costs and 
technical expertise required by conventional methods. Our prototype, showcased through an interactive web application, demonstrates the feasibility and efficiency of generating emotionally r
ich avatars from simple audio inputs, offering a scalable solution that could revolutionize content creation in animation and virtual interactions. Expected outcomes include enhanced lip-sync accuracy, 
and an improved algorithm for differentiating speech from background noise, thereby ensuring precise facial expression generation in accordance with the spoken content. Through this project, 
we aim not only to advance the field of speech-driven facial animation but also to democratize the creation of animated content, making it accessible to a broader range of creators and industries.
